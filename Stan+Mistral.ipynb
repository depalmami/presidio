{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOSDv1sgmOCsgCwavBvSSEl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8e980ca6c3634defaa1bef95cf0a7289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4680dc97f513472294be4c2bcf7a6b6f",
              "IPY_MODEL_73cdfb0dd8ba44ad9b1fe9f6592215df",
              "IPY_MODEL_a963a944c3384d81bae939a9d0c41fc9"
            ],
            "layout": "IPY_MODEL_d3e71137edd142e18ec26e80d43164df"
          }
        },
        "4680dc97f513472294be4c2bcf7a6b6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10f1a678f4714fc0b79aeff7cd96513d",
            "placeholder": "​",
            "style": "IPY_MODEL_aaf21664d3db48ecbf341695b8fb43c6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "73cdfb0dd8ba44ad9b1fe9f6592215df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5d5952bb52543ed90f7f26ae7a2def3",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8707142eb7043b082f651aa379ba24a",
            "value": 2
          }
        },
        "a963a944c3384d81bae939a9d0c41fc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7c6e4f3632c4f0fa5fdc0fe5ce14030",
            "placeholder": "​",
            "style": "IPY_MODEL_d7b2b61532fd40aaa56cd9cb5df23bcb",
            "value": " 2/2 [01:28&lt;00:00, 41.13s/it]"
          }
        },
        "d3e71137edd142e18ec26e80d43164df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10f1a678f4714fc0b79aeff7cd96513d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaf21664d3db48ecbf341695b8fb43c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5d5952bb52543ed90f7f26ae7a2def3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8707142eb7043b082f651aa379ba24a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7c6e4f3632c4f0fa5fdc0fe5ce14030": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7b2b61532fd40aaa56cd9cb5df23bcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/depalmami/presidio/blob/main/Stan%2BMistral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installa le dipendenze necessarie\n",
        "!pip install -q gradio transformers torch presidio-analyzer presidio-anonymizer spacy bitsandbytes accelerate\n",
        "!python -m spacy download it_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSOPdHfCwr_N",
        "outputId": "40ddd0ba-bec5-42b6-a996-b3264ab34ba5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting it-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m118.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8e980ca6c3634defaa1bef95cf0a7289",
            "4680dc97f513472294be4c2bcf7a6b6f",
            "73cdfb0dd8ba44ad9b1fe9f6592215df",
            "a963a944c3384d81bae939a9d0c41fc9",
            "d3e71137edd142e18ec26e80d43164df",
            "10f1a678f4714fc0b79aeff7cd96513d",
            "aaf21664d3db48ecbf341695b8fb43c6",
            "d5d5952bb52543ed90f7f26ae7a2def3",
            "e8707142eb7043b082f651aa379ba24a",
            "b7c6e4f3632c4f0fa5fdc0fe5ce14030",
            "d7b2b61532fd40aaa56cd9cb5df23bcb"
          ]
        },
        "id": "kQy3oiMdvKDD",
        "outputId": "11de10a6-5803-48f6-d4fb-b028fe9378bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caricamento modello Stanford...\n",
            "Modello caricato con successo!\n",
            "Etichette del modello: ['PHONE', 'ID', 'DATE', 'VENDOR', 'PATIENT', 'HOSPITAL', 'HCW']\n",
            "Inizializzazione del modello Mistral...\n",
            "GPU disponibile con 10.89 GB di memoria libera\n",
            "Utilizzo mistralai/Mistral-7B-v0.1 con 4-bit quantization\n",
            "Caricamento modello mistralai/Mistral-7B-v0.1...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e980ca6c3634defaa1bef95cf0a7289"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caricamento tokenizer...\n",
            "Modello mistralai/Mistral-7B-v0.1 inizializzato con successo\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://296af15c9749cee5e0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://296af15c9749cee5e0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello LLM ha impiegato 15.89 secondi per l'analisi\n",
            "Risposta del modello (primi 100 caratteri): ...\n",
            "Nessun JSON trovato nella risposta, tentativo estrazione manuale\n",
            "Modello LLM ha impiegato 15.93 secondi per l'analisi\n",
            "Risposta del modello (primi 100 caratteri): ...\n",
            "Nessun JSON trovato nella risposta, tentativo estrazione manuale\n",
            "Modello LLM ha impiegato 16.12 secondi per l'analisi\n",
            "Risposta del modello (primi 100 caratteri): ...\n",
            "Nessun JSON trovato nella risposta, tentativo estrazione manuale\n",
            "Modello LLM ha impiegato 15.79 secondi per l'analisi\n",
            "Risposta del modello (primi 100 caratteri): ...\n",
            "Nessun JSON trovato nella risposta, tentativo estrazione manuale\n",
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mblock_thread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2982\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2983\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-61f000ec14a3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[0;31m# Avvia l'interfaccia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m     \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, _frontend)\u001b[0m\n\u001b[1;32m   2886\u001b[0m             )\n\u001b[1;32m   2887\u001b[0m         ):\n\u001b[0;32m-> 2888\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2890\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTupleNoPrint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_app\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mblock_thread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2984\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Keyboard interruption in main thread... closing server.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2986\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2987\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtunnel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCURRENT_TUNNELS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m                 \u001b[0mtunnel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/http_server.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;31m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "from presidio_anonymizer.entities import OperatorConfig\n",
        "\n",
        "# Inizializza il tokenizer e modello di Mistral\n",
        "def initialize_mistral():\n",
        "    print(\"Inizializzazione del modello Mistral...\")\n",
        "    try:\n",
        "        # Verifica disponibilità di GPU e memoria\n",
        "        if torch.cuda.is_available():\n",
        "            free_gpu_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)\n",
        "            free_gb = free_gpu_memory / (1024**3)\n",
        "            print(f\"GPU disponibile con {free_gb:.2f} GB di memoria libera\")\n",
        "        else:\n",
        "            print(\"GPU non disponibile, utilizzo CPU\")\n",
        "            free_gb = 0\n",
        "\n",
        "        # Scegli il modello in base alla memoria disponibile\n",
        "        if free_gb > 10:\n",
        "            # Più di 10GB di memoria libera, prova Mistral 7B\n",
        "            model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "            print(f\"Utilizzo {model_name} con 4-bit quantization\")\n",
        "            use_4bit = True\n",
        "        elif free_gb > 5:\n",
        "            # Tra 5 e 10GB, usa un modello più piccolo\n",
        "            model_name = \"google/gemma-2b\"\n",
        "            print(f\"Utilizzo {model_name} con 4-bit quantization\")\n",
        "            use_4bit = True\n",
        "        elif free_gb > 2:\n",
        "            # Memoria molto limitata, usa un modello ancora più piccolo\n",
        "            model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "            print(f\"Utilizzo {model_name} con 8-bit quantization\")\n",
        "            use_4bit = False\n",
        "        else:\n",
        "            # Memoria GPU insufficiente o nessuna GPU\n",
        "            model_name = \"distilgpt2\"\n",
        "            print(f\"Memoria GPU insufficiente, utilizzo {model_name} su CPU\")\n",
        "            use_4bit = False\n",
        "\n",
        "        # Configurazione per la quantizzazione\n",
        "        from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "        # Prepara configurazioni di caricamento in base alla memoria disponibile\n",
        "        load_kwargs = {\n",
        "            \"device_map\": \"auto\" if torch.cuda.is_available() else \"cpu\"\n",
        "        }\n",
        "\n",
        "        if use_4bit and torch.cuda.is_available():\n",
        "            # Configurazione 4-bit\n",
        "            bnb_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.float16\n",
        "            )\n",
        "            load_kwargs[\"quantization_config\"] = bnb_config\n",
        "            load_kwargs[\"torch_dtype\"] = torch.float16\n",
        "        elif torch.cuda.is_available():\n",
        "            # Configurazione 8-bit o standard\n",
        "            load_kwargs[\"torch_dtype\"] = torch.float16\n",
        "\n",
        "        # Aggiungi offload CPU se la memoria è scarsa\n",
        "        if free_gb < 4 and torch.cuda.is_available():\n",
        "            load_kwargs[\"offload_folder\"] = \"offload_folder\"\n",
        "            load_kwargs[\"offload_state_dict\"] = True\n",
        "            print(\"Aggiunta configurazione di offload su CPU/disco\")\n",
        "\n",
        "        # Carica modello e tokenizer separatamente\n",
        "        print(f\"Caricamento modello {model_name}...\")\n",
        "        try:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                **load_kwargs\n",
        "            )\n",
        "\n",
        "            print(\"Caricamento tokenizer...\")\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "            # Inizializza pipeline con modello già configurato\n",
        "            pipe = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                max_new_tokens=512,\n",
        "                do_sample=False\n",
        "            )\n",
        "\n",
        "            print(f\"Modello {model_name} inizializzato con successo\")\n",
        "            return pipe\n",
        "        except (ValueError, RuntimeError) as e:\n",
        "            print(f\"Errore durante il caricamento del modello {model_name}: {e}\")\n",
        "            # Se il modello è fallito e non è già distilgpt2, prova con distilgpt2\n",
        "            if model_name != \"distilgpt2\":\n",
        "                print(\"Tentativo con distilgpt2 (modello più piccolo)\")\n",
        "                return initialize_gpt2_fallback()\n",
        "            else:\n",
        "                raise e\n",
        "    except Exception as e:\n",
        "        print(f\"Errore critico nell'inizializzazione del modello: {e}\")\n",
        "        return None\n",
        "\n",
        "# Inizializzazione del modello fallback GPT-2 distillato (molto leggero)\n",
        "def initialize_gpt2_fallback():\n",
        "    try:\n",
        "        print(\"Tentativo di caricamento del modello minimo distilgpt2...\")\n",
        "\n",
        "        # Carica senza quantizzazione né GPU se necessario\n",
        "        model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # Usa sempre CPU per il fallback\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            device=-1,  # Forza CPU\n",
        "            max_new_tokens=128,  # Ridotto per evitare timeout\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "        print(\"Modello distilgpt2 caricato con successo su CPU\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        print(f\"Errore critico anche con distilgpt2: {e}\")\n",
        "        print(\"Nessun modello LLM disponibile, il sistema userà solo il modello Stanford\")\n",
        "        return None\n",
        "\n",
        "# Crea un riconoscitore personalizzato basato sul modello Stanford\n",
        "class StanfordDeidentifier:\n",
        "    def __init__(self):\n",
        "        print(\"Caricamento modello Stanford...\")\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\"StanfordAIMI/stanford-deidentifier-base\")\n",
        "            self.model = AutoModelForTokenClassification.from_pretrained(\"StanfordAIMI/stanford-deidentifier-base\")\n",
        "            print(\"Modello caricato con successo!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello Stanford: {e}\")\n",
        "            print(\"Tentativo con nome alternativo del modello...\")\n",
        "            try:\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(\"stanfordaimi/stanford-deidentifier-base\")\n",
        "                self.model = AutoModelForTokenClassification.from_pretrained(\"stanfordaimi/stanford-deidentifier-base\")\n",
        "                print(\"Modello caricato con successo!\")\n",
        "            except Exception as e:\n",
        "                print(f\"Errore nel caricamento del modello alternativo: {e}\")\n",
        "                print(\"Caricamento di un modello NER generico...\")\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "                self.model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
        "                print(\"Modello NER generico caricato come fallback!\")\n",
        "\n",
        "        # Ottieni le etichette del modello\n",
        "        self.id2label = self.model.config.id2label\n",
        "        self.label2id = self.model.config.label2id\n",
        "        print(f\"Etichette del modello: {list(set([l.split('-')[1] if '-' in l else l for l in self.id2label.values() if l != 'O']))}\")\n",
        "\n",
        "    def analyze(self, text, threshold=0.5):\n",
        "        # Tokenizzazione\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "        # Predizione\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        # Processa le predizioni\n",
        "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predictions = torch.argmax(probabilities, dim=-1)\n",
        "\n",
        "        # Mappatura approssimativa token -> testo originale\n",
        "        token_map = []\n",
        "        tokens = self.tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
        "\n",
        "        # Pre-processing per mappare token ai caratteri\n",
        "        current_pos = 0\n",
        "        for token in tokens:\n",
        "            # Ignora token speciali\n",
        "            if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\", \"<s>\", \"</s>\", \"<pad>\"]:\n",
        "                token_map.append((-1, -1))\n",
        "                continue\n",
        "\n",
        "            # Gestisci token normali\n",
        "            clean_token = token.replace(\"##\", \"\").replace(\"Ġ\", \"\")\n",
        "            if not clean_token:  # Skip token vuoti\n",
        "                token_map.append((-1, -1))\n",
        "                continue\n",
        "\n",
        "            # Cerca nel testo\n",
        "            pos = text.find(clean_token, current_pos)\n",
        "            if pos >= 0:\n",
        "                token_map.append((pos, pos + len(clean_token)))\n",
        "                current_pos = pos + len(clean_token)\n",
        "            else:\n",
        "                # Se non trovato, cerca senza case sensitivity\n",
        "                pos = text.lower().find(clean_token.lower(), current_pos)\n",
        "                if pos >= 0:\n",
        "                    token_map.append((pos, pos + len(clean_token)))\n",
        "                    current_pos = pos + len(clean_token)\n",
        "                else:\n",
        "                    token_map.append((-1, -1))\n",
        "\n",
        "        # Estrai entità\n",
        "        entities = []\n",
        "        current_entity = None\n",
        "\n",
        "        for i, pred_id in enumerate(predictions[0]):\n",
        "            if i >= len(token_map):\n",
        "                continue\n",
        "\n",
        "            pred_label = self.id2label[pred_id.item()]\n",
        "            confidence = probabilities[0, i, pred_id].item()\n",
        "\n",
        "            # Salta token speciali o sotto la soglia\n",
        "            if token_map[i][0] == -1 or confidence < threshold or pred_label == \"O\":\n",
        "                if current_entity is not None:\n",
        "                    entities.append(current_entity)\n",
        "                    current_entity = None\n",
        "                continue\n",
        "\n",
        "            # Inizio nuova entità\n",
        "            if pred_label.startswith(\"B-\"):\n",
        "                if current_entity is not None:\n",
        "                    entities.append(current_entity)\n",
        "\n",
        "                entity_type = pred_label[2:]  # rimuovi \"B-\"\n",
        "                current_entity = {\n",
        "                    \"entity_type\": entity_type,\n",
        "                    \"start\": token_map[i][0],\n",
        "                    \"end\": token_map[i][1],\n",
        "                    \"text\": text[token_map[i][0]:token_map[i][1]],\n",
        "                    \"score\": confidence\n",
        "                }\n",
        "\n",
        "            # Continuazione entità\n",
        "            elif pred_label.startswith(\"I-\") and current_entity is not None:\n",
        "                if token_map[i][0] >= 0:\n",
        "                    current_entity[\"end\"] = token_map[i][1]\n",
        "                    current_entity[\"text\"] = text[current_entity[\"start\"]:current_entity[\"end\"]]\n",
        "                    current_entity[\"score\"] = (current_entity[\"score\"] + confidence) / 2\n",
        "\n",
        "        # Aggiungi l'ultima entità\n",
        "        if current_entity is not None:\n",
        "            entities.append(current_entity)\n",
        "\n",
        "        # In caso di errori nella mappatura, usa metodo basato su regex come fallback\n",
        "        if not entities:\n",
        "            entities = self.analyze_with_regex(text)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    # Fallback con regex se il modello non funziona correttamente\n",
        "    def analyze_with_regex(self, text):\n",
        "        entities = []\n",
        "\n",
        "        # Rileva nomi di persona (formato: parole che iniziano con maiuscola)\n",
        "        for match in re.finditer(r'\\b[A-Z][a-z]+ [A-Z][a-z]+\\b', text):\n",
        "            entities.append({\n",
        "                \"entity_type\": \"PER\",\n",
        "                \"start\": match.start(),\n",
        "                \"end\": match.end(),\n",
        "                \"text\": match.group(),\n",
        "                \"score\": 0.85\n",
        "            })\n",
        "\n",
        "        # Rileva date\n",
        "        for match in re.finditer(r'\\b\\d{1,2}[\\/\\-\\.]\\d{1,2}[\\/\\-\\.]\\d{2,4}\\b', text):\n",
        "            entities.append({\n",
        "                \"entity_type\": \"DATE\",\n",
        "                \"start\": match.start(),\n",
        "                \"end\": match.end(),\n",
        "                \"text\": match.group(),\n",
        "                \"score\": 0.9\n",
        "            })\n",
        "\n",
        "        # Rileva email\n",
        "        for match in re.finditer(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text):\n",
        "            entities.append({\n",
        "                \"entity_type\": \"EMAIL\",\n",
        "                \"start\": match.start(),\n",
        "                \"end\": match.end(),\n",
        "                \"text\": match.group(),\n",
        "                \"score\": 0.95\n",
        "            })\n",
        "\n",
        "        # Rileva numeri di telefono\n",
        "        for match in re.finditer(r'\\b\\+?[0-9]{10,15}\\b|\\b\\+?[0-9]{2,4}[- ][0-9]{5,10}\\b', text):\n",
        "            entities.append({\n",
        "                \"entity_type\": \"PHONE\",\n",
        "                \"start\": match.start(),\n",
        "                \"end\": match.end(),\n",
        "                \"text\": match.group(),\n",
        "                \"score\": 0.9\n",
        "            })\n",
        "\n",
        "        return entities\n",
        "\n",
        "# Funzione per il refinement tramite Mistral\n",
        "def refine_with_mistral(text, entities, mistral_pipe):\n",
        "    if not mistral_pipe:\n",
        "        print(\"Mistral non disponibile, skipping refinement\")\n",
        "        return entities, []\n",
        "\n",
        "    # Salva le entità originali per confronto\n",
        "    original_entities = entities.copy()\n",
        "\n",
        "    # Prepara un contesto con le entità già estratte\n",
        "    entities_context = \"\"\n",
        "    for i, entity in enumerate(entities):\n",
        "        entities_context += f\"{i+1}. Tipo: {entity['entity_type']}, Testo: '{entity['text']}'\\n\"\n",
        "\n",
        "    prompt = f\"\"\"Sei un assistente specializzato nell'anonimizzazione dei dati personali.\n",
        "Nel testo seguente, ho già identificato alcune informazioni sensibili, ma potrebbero essercene altre che ho perso.\n",
        "\n",
        "Testo da analizzare:\n",
        "{text}\n",
        "\n",
        "Entità già identificate:\n",
        "{entities_context}\n",
        "\n",
        "Per favore, identifica e aggiungi SOLO altre entità sensibili che NON ho già trovato, come:\n",
        "- Nomi e cognomi di persone (PERSON)\n",
        "- Codici fiscali italiani (CF) - formato: 16 caratteri alfanumerici\n",
        "- Partite IVA italiane (PIVA) - formato: IT seguito da 11 cifre\n",
        "- Date di nascita o altre date rilevanti (DATE)\n",
        "- Indirizzi (LOCATION)\n",
        "- Numeri di telefono (PHONE)\n",
        "- IBAN o altri identificativi bancari (IBAN) - formato: IT seguito da caratteri alfanumerici\n",
        "- Targhe di veicoli (TARGA) - formato italiano: 2 lettere, 3 numeri, 2 lettere\n",
        "- Email (EMAIL)\n",
        "\n",
        "Presta particolare attenzione a codici fiscali, IBAN, partite IVA e targhe che potrebbero essere stati ignorati.\n",
        "\n",
        "Fornisci il risultato in questo formato JSON:\n",
        "[\n",
        "  {{\n",
        "    \"entity_type\": \"TIPO_ENTITÀ\",\n",
        "    \"text\": \"testo dell'entità\",\n",
        "    \"reason\": \"motivo dell'identificazione\"\n",
        "  }}\n",
        "]\n",
        "\n",
        "Se non ci sono altre entità da aggiungere, restituisci un array vuoto: []\n",
        "\"\"\"\n",
        "\n",
        "    # Lista per tenere traccia delle entità aggiunte da Mistral\n",
        "    added_entities = []\n",
        "\n",
        "    # Ottieni la risposta da Mistral\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        response = mistral_pipe(prompt, temperature=0.1, max_new_tokens=256)[0]['generated_text'].replace(prompt, \"\").strip()\n",
        "        end_time = time.time()\n",
        "        print(f\"Modello LLM ha impiegato {end_time - start_time:.2f} secondi per l'analisi\")\n",
        "\n",
        "        # Debug: mostra parte della risposta per diagnostica\n",
        "        print(f\"Risposta del modello (primi 100 caratteri): {response[:100]}...\")\n",
        "\n",
        "        # Estrai la parte JSON dalla risposta\n",
        "        json_match = re.search(r'\\[\\s*{.*?}\\s*(?:,\\s*{.*?}\\s*)*\\]', response, re.DOTALL)\n",
        "        if json_match:\n",
        "            json_str = json_match.group(0)\n",
        "            try:\n",
        "                # Pulisci il JSON per renderlo valido se necessario\n",
        "                json_str = json_str.replace(\"'\", '\"')  # Sostituisci apici singoli con doppi\n",
        "                json_str = re.sub(r',\\s*\\]', ']', json_str)  # Rimuovi virgole finali\n",
        "\n",
        "                print(f\"Tentativo parsing JSON: {json_str[:100]}...\")\n",
        "                additional_entities = json.loads(json_str)\n",
        "\n",
        "                # Aggiungi le nuove entità, trovando le posizioni nel testo\n",
        "                new_entities_added = 0\n",
        "                for new_entity in additional_entities:\n",
        "                    entity_text = new_entity.get(\"text\", \"\")\n",
        "                    entity_type = new_entity.get(\"entity_type\", \"UNKNOWN\")\n",
        "\n",
        "                    # Normalizza il tipo di entità\n",
        "                    if entity_type.upper() in [\"NOME E COGNOME\", \"PERSON\", \"PERSONA\", \"NOME COGNOME\", \"NOME_COGNOME\"]:\n",
        "                        entity_type = \"PERSON\"\n",
        "                    elif entity_type.upper() in [\"TELEFONO\", \"NUMERO TELEFONO\", \"NUMERO DI TELEFONO\", \"NUMERO_TELEFONO\"]:\n",
        "                        entity_type = \"PHONE\"\n",
        "                    elif entity_type.upper() in [\"DATA\", \"DATA DI NASCITA\", \"DATA_NASCITA\"]:\n",
        "                        entity_type = \"DATE\"\n",
        "                    elif entity_type.upper() in [\"INDIRIZZO\", \"LOCATION\", \"LOCALITÀ\"]:\n",
        "                        entity_type = \"LOCATION\"\n",
        "                    elif entity_type.upper() in [\"CODICE FISCALE\", \"CF\", \"CODICE_FISCALE\"]:\n",
        "                        entity_type = \"CF\"\n",
        "                    elif entity_type.upper() in [\"PARTITA IVA\", \"PIVA\", \"P_IVA\", \"PARTITA_IVA\"]:\n",
        "                        entity_type = \"PIVA\"\n",
        "                    elif entity_type.upper() in [\"IBAN\", \"IBAN_CODE\", \"CODICE IBAN\"]:\n",
        "                        entity_type = \"IBAN\"\n",
        "\n",
        "                    # Se il testo è vuoto, salta\n",
        "                    if not entity_text:\n",
        "                        continue\n",
        "\n",
        "                    # Cerca la posizione nel testo\n",
        "                    start_pos = text.find(entity_text)\n",
        "                    if start_pos >= 0:\n",
        "                        # Verifica che l'entità non sia già presente o sovrapposta\n",
        "                        is_duplicate = False\n",
        "                        is_overlapping = False\n",
        "\n",
        "                        for existing_entity in entities:\n",
        "                            # Controlla duplicati esatti\n",
        "                            if (existing_entity[\"text\"] == entity_text and\n",
        "                                existing_entity[\"entity_type\"] == entity_type):\n",
        "                                is_duplicate = True\n",
        "                                break\n",
        "\n",
        "                            # Controlla sovrapposizioni\n",
        "                            new_start = start_pos\n",
        "                            new_end = start_pos + len(entity_text)\n",
        "                            existing_start = existing_entity[\"start\"]\n",
        "                            existing_end = existing_entity[\"end\"]\n",
        "\n",
        "                            # Due intervalli si sovrappongono se l'inizio di uno è minore della fine dell'altro\n",
        "                            # e la fine di uno è maggiore dell'inizio dell'altro\n",
        "                            if (new_start < existing_end and new_end > existing_start):\n",
        "                                is_overlapping = True\n",
        "                                break\n",
        "\n",
        "                        if not is_duplicate and not is_overlapping:\n",
        "                            new_entity = {\n",
        "                                \"entity_type\": entity_type,\n",
        "                                \"start\": start_pos,\n",
        "                                \"end\": start_pos + len(entity_text),\n",
        "                                \"text\": entity_text,\n",
        "                                \"score\": 0.85,  # Score fisso per le entità LLM\n",
        "                                \"source\": \"llm\",\n",
        "                                \"reason\": new_entity.get(\"reason\", \"Identificato da LLM\")\n",
        "                            }\n",
        "\n",
        "                            entities.append(new_entity)\n",
        "                            added_entities.append(new_entity)\n",
        "                            new_entities_added += 1\n",
        "\n",
        "                print(f\"LLM ha identificato {new_entities_added} nuove entità valide\")\n",
        "\n",
        "            except json.JSONDecodeError as je:\n",
        "                print(f\"Errore nel parsing JSON dalla risposta: {je}\")\n",
        "                # Tentativo di estrarre manualmente i dati dalla risposta testuale\n",
        "                try:\n",
        "                    # Cerca pattern di entità nella risposta testuale\n",
        "                    added_entities = manual_extraction(text, response, entities)\n",
        "                except Exception as ex:\n",
        "                    print(f\"Fallito anche il parsing manuale: {ex}\")\n",
        "        else:\n",
        "            print(\"Nessun JSON trovato nella risposta, tentativo estrazione manuale\")\n",
        "            added_entities = manual_extraction(text, response, entities)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante l'analisi con LLM: {e}\")\n",
        "\n",
        "    return entities, added_entities\n",
        "\n",
        "# Funzione ausiliaria per estrarre manualmente entità dalla risposta testuale\n",
        "def manual_extraction(text, response, entities):\n",
        "    # Lista per tenere traccia delle entità aggiunte manualmente\n",
        "    added_entities = []\n",
        "\n",
        "    # Cerca pattern comuni per codici fiscali italiani\n",
        "    cf_pattern = r'[A-Z]{6}\\d{2}[A-Z]\\d{2}[A-Z]\\d{3}[A-Z]'\n",
        "    for match in re.finditer(cf_pattern, text):  # Nota: cerchiamo direttamente nel testo originale, non nella risposta\n",
        "        cf_text = match.group(0)\n",
        "        start_pos = match.start()\n",
        "\n",
        "        # Verifica che l'entità non sia già presente\n",
        "        is_duplicate = False\n",
        "        for existing_entity in entities:\n",
        "            if (existing_entity[\"text\"] == cf_text and\n",
        "                existing_entity[\"entity_type\"] == \"CF\"):\n",
        "                is_duplicate = True\n",
        "                break\n",
        "\n",
        "        if not is_duplicate:\n",
        "            new_entity = {\n",
        "                \"entity_type\": \"CF\",\n",
        "                \"start\": start_pos,\n",
        "                \"end\": start_pos + len(cf_text),\n",
        "                \"text\": cf_text,\n",
        "                \"score\": 0.9,\n",
        "                \"source\": \"regex\",\n",
        "                \"reason\": \"Estratto da pattern di codice fiscale\"\n",
        "            }\n",
        "            entities.append(new_entity)\n",
        "            added_entities.append(new_entity)\n",
        "\n",
        "    # Cerca IBAN italiani\n",
        "    iban_pattern = r'IT\\d{2}[A-Z0-9]{10,30}'\n",
        "    for match in re.finditer(iban_pattern, text):\n",
        "        iban_text = match.group(0)\n",
        "        start_pos = match.start()\n",
        "\n",
        "        # Verifica che l'entità non sia già presente\n",
        "        is_duplicate = False\n",
        "        for existing_entity in entities:\n",
        "            if (existing_entity[\"text\"] == iban_text and\n",
        "                existing_entity[\"entity_type\"] == \"IBAN\"):\n",
        "                is_duplicate = True\n",
        "                break\n",
        "\n",
        "        if not is_duplicate:\n",
        "            new_entity = {\n",
        "                \"entity_type\": \"IBAN\",\n",
        "                \"start\": start_pos,\n",
        "                \"end\": start_pos + len(iban_text),\n",
        "                \"text\": iban_text,\n",
        "                \"score\": 0.9,\n",
        "                \"source\": \"regex\",\n",
        "                \"reason\": \"Estratto da pattern di IBAN\"\n",
        "            }\n",
        "            entities.append(new_entity)\n",
        "            added_entities.append(new_entity)\n",
        "\n",
        "    # Cerca date nel formato gg/mm/aaaa\n",
        "    date_pattern = r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b'\n",
        "    for match in re.finditer(date_pattern, text):\n",
        "        date_text = match.group(0)\n",
        "        start_pos = match.start()\n",
        "\n",
        "        # Verifica che l'entità non sia già presente\n",
        "        is_duplicate = False\n",
        "        for existing_entity in entities:\n",
        "            if (existing_entity[\"text\"] == date_text and\n",
        "                existing_entity[\"entity_type\"] == \"DATE\"):\n",
        "                is_duplicate = True\n",
        "                break\n",
        "\n",
        "        if not is_duplicate:\n",
        "            new_entity = {\n",
        "                \"entity_type\": \"DATE\",\n",
        "                \"start\": start_pos,\n",
        "                \"end\": start_pos + len(date_text),\n",
        "                \"text\": date_text,\n",
        "                \"score\": 0.9,\n",
        "                \"source\": \"regex\",\n",
        "                \"reason\": \"Estratto da pattern di data\"\n",
        "            }\n",
        "            entities.append(new_entity)\n",
        "            added_entities.append(new_entity)\n",
        "\n",
        "    # Cerca targhe nel formato AA000BB\n",
        "    targa_pattern = r'\\b[A-Z]{2}\\d{3}[A-Z]{2}\\b'\n",
        "    for match in re.finditer(targa_pattern, text):\n",
        "        targa_text = match.group(0)\n",
        "        start_pos = match.start()\n",
        "\n",
        "        # Verifica che l'entità non sia già presente\n",
        "        is_duplicate = False\n",
        "        for existing_entity in entities:\n",
        "            if (existing_entity[\"text\"] == targa_text and\n",
        "                existing_entity[\"entity_type\"] == \"TARGA\"):\n",
        "                is_duplicate = True\n",
        "                break\n",
        "\n",
        "        if not is_duplicate:\n",
        "            new_entity = {\n",
        "                \"entity_type\": \"TARGA\",\n",
        "                \"start\": start_pos,\n",
        "                \"end\": start_pos + len(targa_text),\n",
        "                \"text\": targa_text,\n",
        "                \"score\": 0.9,\n",
        "                \"source\": \"regex\",\n",
        "                \"reason\": \"Estratto da pattern di targa\"\n",
        "            }\n",
        "            entities.append(new_entity)\n",
        "            added_entities.append(new_entity)\n",
        "\n",
        "    # Cerca Partite IVA italiane\n",
        "    piva_pattern = r'\\b(IT)?\\d{11}\\b'\n",
        "    for match in re.finditer(piva_pattern, text):\n",
        "        piva_text = match.group(0)\n",
        "        start_pos = match.start()\n",
        "\n",
        "        # Verifica che l'entità non sia già presente\n",
        "        is_duplicate = False\n",
        "        for existing_entity in entities:\n",
        "            if (existing_entity[\"text\"] == piva_text and\n",
        "                existing_entity[\"entity_type\"] == \"PIVA\"):\n",
        "                is_duplicate = True\n",
        "                break\n",
        "\n",
        "        if not is_duplicate:\n",
        "            new_entity = {\n",
        "                \"entity_type\": \"PIVA\",\n",
        "                \"start\": start_pos,\n",
        "                \"end\": start_pos + len(piva_text),\n",
        "                \"text\": piva_text,\n",
        "                \"score\": 0.9,\n",
        "                \"source\": \"regex\",\n",
        "                \"reason\": \"Estratto da pattern di partita IVA\"\n",
        "            }\n",
        "            entities.append(new_entity)\n",
        "            added_entities.append(new_entity)\n",
        "\n",
        "    # Cerca numeri di telefono\n",
        "    phone_pattern = r'\\b\\+?\\d{2,4}[-\\s]?\\d{3,10}[-\\s]?\\d{3,10}\\b'\n",
        "    for match in re.finditer(phone_pattern, text):\n",
        "        phone_text = match.group(0)\n",
        "        start_pos = match.start()\n",
        "\n",
        "        # Verifica che l'entità non sia già presente\n",
        "        is_duplicate = False\n",
        "        for existing_entity in entities:\n",
        "            if (existing_entity[\"text\"] == phone_text and\n",
        "                existing_entity[\"entity_type\"] == \"PHONE\"):\n",
        "                is_duplicate = True\n",
        "                break\n",
        "\n",
        "        if not is_duplicate:\n",
        "            new_entity = {\n",
        "                \"entity_type\": \"PHONE\",\n",
        "                \"start\": start_pos,\n",
        "                \"end\": start_pos + len(phone_text),\n",
        "                \"text\": phone_text,\n",
        "                \"score\": 0.9,\n",
        "                \"source\": \"regex\",\n",
        "                \"reason\": \"Estratto da pattern di telefono\"\n",
        "            }\n",
        "            entities.append(new_entity)\n",
        "            added_entities.append(new_entity)\n",
        "\n",
        "    # Riporta la lista delle entità aggiunte\n",
        "    return added_entities\n",
        "\n",
        "# Definizione dei colori per le entità\n",
        "entity_colors = {\n",
        "    \"PER\": \"#fecaca\",\n",
        "    \"PERSON\": \"#fecaca\",\n",
        "    \"NOME\": \"#fecaca\",\n",
        "    \"COGNOME\": \"#fecaca\",\n",
        "    \"PHONE\": \"#fde68a\",\n",
        "    \"TELEFONO\": \"#fde68a\",\n",
        "    \"EMAIL\": \"#fed7aa\",\n",
        "    \"DATE\": \"#fcd34d\",\n",
        "    \"DATA\": \"#fcd34d\",\n",
        "    \"LOC\": \"#fbcfe8\",\n",
        "    \"LOCATION\": \"#fbcfe8\",\n",
        "    \"INDIRIZZO\": \"#fbcfe8\",\n",
        "    \"ORG\": \"#93c5fd\",\n",
        "    \"ORGANIZATION\": \"#93c5fd\",\n",
        "    \"ID\": \"#ddd6fe\",\n",
        "    \"CF\": \"#ddd6fe\",\n",
        "    \"PIVA\": \"#ddd6fe\",\n",
        "    \"CODICE_FISCALE\": \"#ddd6fe\",\n",
        "    \"PARTITA_IVA\": \"#ddd6fe\",\n",
        "    \"IBAN\": \"#1e90ff\",\n",
        "    \"IBAN_CODE\": \"#1e90ff\",\n",
        "    \"MEDICALRECORD\": \"#6ee7b7\",\n",
        "    \"AGE\": \"#fdba74\",\n",
        "    \"DOCTOR\": \"#c4b5fd\",\n",
        "    \"PATIENT\": \"#fecaca\",\n",
        "    \"HOSPITAL\": \"#93c5fd\",\n",
        "    \"TARGA\": \"#bdb76b\"\n",
        "}\n",
        "\n",
        "entity_names = {\n",
        "    \"PER\": \"Nome Persona\",\n",
        "    \"PERSON\": \"Nome Persona\",\n",
        "    \"NOME\": \"Nome\",\n",
        "    \"COGNOME\": \"Cognome\",\n",
        "    \"PHONE\": \"Numero di Telefono\",\n",
        "    \"TELEFONO\": \"Numero di Telefono\",\n",
        "    \"EMAIL\": \"Email\",\n",
        "    \"DATE\": \"Data\",\n",
        "    \"DATA\": \"Data\",\n",
        "    \"LOC\": \"Località\",\n",
        "    \"INDIRIZZO\": \"Indirizzo\",\n",
        "    \"ORG\": \"Organizzazione\",\n",
        "    \"ORGANIZATION\": \"Organizzazione\",\n",
        "    \"ID\": \"Identificativo\",\n",
        "    \"CF\": \"Codice Fiscale\",\n",
        "    \"PIVA\": \"Partita IVA\",\n",
        "    \"CODICE_FISCALE\": \"Codice Fiscale\",\n",
        "    \"PARTITA_IVA\": \"Partita IVA\",\n",
        "    \"IBAN\": \"IBAN\",\n",
        "    \"IBAN_CODE\": \"IBAN\",\n",
        "    \"MEDICALRECORD\": \"Cartella Medica\",\n",
        "    \"AGE\": \"Età\",\n",
        "    \"DOCTOR\": \"Medico\",\n",
        "    \"PATIENT\": \"Paziente\",\n",
        "    \"HOSPITAL\": \"Ospedale\",\n",
        "    \"TARGA\": \"Targa Veicolo\"\n",
        "}\n",
        "\n",
        "# Funzione per evidenziare le entità in HTML\n",
        "def highlight_entities_html(text, entities):\n",
        "    if not entities:\n",
        "        return text\n",
        "\n",
        "    # Prepariamo HTML con span colorati\n",
        "    chars = list(text)\n",
        "    spans = []\n",
        "\n",
        "    for entity in entities:\n",
        "        entity_type = entity[\"entity_type\"]\n",
        "        color = entity_colors.get(entity_type, \"#cccccc\")\n",
        "        name = entity_names.get(entity_type, entity_type)\n",
        "        score = int(entity.get(\"score\", 0.8) * 100)\n",
        "        source = entity.get(\"source\", \"stanford\")\n",
        "\n",
        "        spans.append({\n",
        "            \"index\": entity[\"start\"],\n",
        "            \"content\": f'<span style=\"background-color: {color}; padding: 2px; border-radius: 3px;\" title=\"{name} ({score}%) - {source}\">',\n",
        "            \"is_opening\": True\n",
        "        })\n",
        "\n",
        "        spans.append({\n",
        "            \"index\": entity[\"end\"],\n",
        "            \"content\": '</span>',\n",
        "            \"is_opening\": False\n",
        "        })\n",
        "\n",
        "    # Ordina i span (chiusura prima dell'apertura se stesso indice)\n",
        "    spans.sort(key=lambda x: (x[\"index\"], not x[\"is_opening\"]))\n",
        "\n",
        "    # Inserisce i tag span nel testo\n",
        "    offset = 0\n",
        "    for span in spans:\n",
        "        adjusted_index = span[\"index\"] + offset\n",
        "        if 0 <= adjusted_index <= len(chars):  # Controlla indici validi\n",
        "            chars.insert(adjusted_index, span[\"content\"])\n",
        "            offset += 1\n",
        "\n",
        "    return \"\".join(chars)\n",
        "\n",
        "# Funzioni di anonimizzazione\n",
        "def anonymize_text(text, entities, anonymization_type=\"replace\"):\n",
        "    # Usa implementazione personalizzata invece di Presidio\n",
        "    if not entities:\n",
        "        return text\n",
        "\n",
        "    # Ordina le entità per posizione di fine in ordine decrescente\n",
        "    sorted_entities = sorted(entities, key=lambda x: x[\"end\"], reverse=True)\n",
        "\n",
        "    if anonymization_type == \"replace\":\n",
        "        # Sostituisci con tag\n",
        "        anonymized = text\n",
        "        for entity in sorted_entities:\n",
        "            anonymized = (\n",
        "                anonymized[:entity[\"start\"]] +\n",
        "                f\"<{entity['entity_type']}>\" +\n",
        "                anonymized[entity[\"end\"]:]\n",
        "            )\n",
        "\n",
        "    elif anonymization_type == \"redact\":\n",
        "        # Sostituisci con asterischi\n",
        "        anonymized = text\n",
        "        for entity in sorted_entities:\n",
        "            redaction_length = entity[\"end\"] - entity[\"start\"]\n",
        "            redaction = \"*\" * redaction_length\n",
        "            anonymized = (\n",
        "                anonymized[:entity[\"start\"]] +\n",
        "                redaction +\n",
        "                anonymized[entity[\"end\"]:]\n",
        "            )\n",
        "\n",
        "    else:  # pseudonymize\n",
        "        # Conta le occorrenze per tipo\n",
        "        type_counts = {}\n",
        "        entity_to_pseudonym = {}\n",
        "\n",
        "        # Crea pseudonimi\n",
        "        for entity in entities:\n",
        "            entity_type = entity[\"entity_type\"]\n",
        "            entity_text = entity[\"text\"]\n",
        "\n",
        "            if entity_type not in type_counts:\n",
        "                type_counts[entity_type] = 0\n",
        "\n",
        "            if entity_text not in entity_to_pseudonym:\n",
        "                type_counts[entity_type] += 1\n",
        "\n",
        "                # Genera pseudonimo\n",
        "                if entity_type in [\"PER\", \"PERSON\", \"PATIENT\", \"NOME\", \"COGNOME\"]:\n",
        "                    pseudonym = f\"Persona {type_counts[entity_type]}\"\n",
        "                elif entity_type == \"DOCTOR\":\n",
        "                    pseudonym = f\"Medico {type_counts[entity_type]}\"\n",
        "                elif entity_type == \"EMAIL\":\n",
        "                    pseudonym = f\"email{type_counts[entity_type]}@example.com\"\n",
        "                elif entity_type in [\"PHONE\", \"TELEFONO\"]:\n",
        "                    pseudonym = f\"+xx-xxx-{str(type_counts[entity_type]).zfill(4)}\"\n",
        "                elif entity_type in [\"ORG\", \"ORGANIZATION\", \"HOSPITAL\"]:\n",
        "                    pseudonym = f\"Organizzazione {type_counts[entity_type]}\"\n",
        "                elif entity_type in [\"DATE\", \"DATA\"]:\n",
        "                    pseudonym = \"GG/MM/AAAA\"\n",
        "                elif entity_type == \"AGE\":\n",
        "                    pseudonym = \"XX anni\"\n",
        "                elif entity_type in [\"CF\", \"CODICE_FISCALE\"]:\n",
        "                    pseudonym = f\"CODFIS{type_counts[entity_type]}\"\n",
        "                elif entity_type in [\"PIVA\", \"PARTITA_IVA\"]:\n",
        "                    pseudonym = f\"PIVA{type_counts[entity_type]}\"\n",
        "                elif entity_type == \"MEDICALRECORD\":\n",
        "                    pseudonym = f\"ID-XXXXX{type_counts[entity_type]}\"\n",
        "                elif entity_type in [\"LOC\", \"LOCATION\", \"INDIRIZZO\"]:\n",
        "                    pseudonym = f\"Luogo {type_counts[entity_type]}\"\n",
        "                elif entity_type in [\"IBAN\", \"IBAN_CODE\"]:\n",
        "                    pseudonym = f\"ITXX-XXXX-XXXX-{type_counts[entity_type]}\"\n",
        "                elif entity_type == \"TARGA\":\n",
        "                    pseudonym = f\"TARGA{type_counts[entity_type]}\"\n",
        "                else:\n",
        "                    pseudonym = f\"{entity_type} {type_counts[entity_type]}\"\n",
        "\n",
        "                entity_to_pseudonym[entity_text] = pseudonym\n",
        "\n",
        "        # Sostituisci entità\n",
        "        anonymized = text\n",
        "        for entity in sorted_entities:\n",
        "            pseudonym = entity_to_pseudonym[entity[\"text\"]]\n",
        "            anonymized = (\n",
        "                anonymized[:entity[\"start\"]] +\n",
        "                pseudonym +\n",
        "                anonymized[entity[\"end\"]:]\n",
        "            )\n",
        "\n",
        "    return anonymized\n",
        "\n",
        "def process_text(text, anonymization_type, threshold, use_llm):\n",
        "    \"\"\"\n",
        "    Elabora il testo analizzandolo con Stanford e opzionalmente con LLM\n",
        "    \"\"\"\n",
        "    # Analizza il testo con il modello Stanford\n",
        "    start_time = time.time()\n",
        "    entities = stanford_model.analyze(text, threshold)\n",
        "    stanford_time = time.time() - start_time\n",
        "\n",
        "    # Refine con LLM se richiesto\n",
        "    llm_time = 0\n",
        "    added_entities = []\n",
        "    if use_llm and mistral_pipe:\n",
        "        start_time = time.time()\n",
        "        entities, added_entities = refine_with_mistral(text, entities, mistral_pipe)\n",
        "        llm_time = time.time() - start_time\n",
        "\n",
        "    # Genera il testo HTML con entità evidenziate\n",
        "    highlighted = highlight_entities_html(text, entities)\n",
        "\n",
        "    # Anonimizza il testo\n",
        "    anonymized = anonymize_text(text, entities, anonymization_type)\n",
        "\n",
        "    # Calcola statistiche\n",
        "    type_count = {}\n",
        "    source_count = {\"stanford\": 0, \"llm\": 0, \"regex\": 0}\n",
        "\n",
        "    for entity in entities:\n",
        "        entity_type = entity[\"entity_type\"]\n",
        "        type_count[entity_type] = type_count.get(entity_type, 0) + 1\n",
        "\n",
        "        source = entity.get(\"source\", \"stanford\")\n",
        "        source_count[source] = source_count.get(source, 0) + 1\n",
        "\n",
        "    avg_confidence = sum(entity.get(\"score\", 0.8) for entity in entities) / len(entities) if entities else 0\n",
        "\n",
        "    # Crea tabella entità in formato HTML per una migliore visualizzazione\n",
        "    entity_table = \"<table style='width:100%; border-collapse: collapse;'>\"\n",
        "    entity_table += \"<tr style='background-color: #f2f2f2;'><th>Tipo</th><th>Testo</th><th>Fonte</th><th>Confidenza</th></tr>\"\n",
        "\n",
        "    # Ordina le entità per posizione nel testo\n",
        "    sorted_entities = sorted(entities, key=lambda x: x[\"start\"])\n",
        "\n",
        "    for entity in sorted_entities:\n",
        "        entity_type = entity[\"entity_type\"]\n",
        "        source = entity.get(\"source\", \"stanford\")\n",
        "        confidence = entity.get(\"score\", 0.8) * 100\n",
        "\n",
        "        # Colore di sfondo in base alla fonte\n",
        "        bg_color = \"#ffffff\"\n",
        "        if source == \"stanford\":\n",
        "            bg_color = \"#e6f7ff\"  # Azzurro chiaro\n",
        "        elif source == \"llm\":\n",
        "            bg_color = \"#f6ffed\"  # Verde chiaro\n",
        "        elif source == \"regex\":\n",
        "            bg_color = \"#fff7e6\"  # Giallo chiaro\n",
        "\n",
        "        entity_table += f\"<tr style='background-color: {bg_color};'>\"\n",
        "        entity_table += f\"<td>{entity_type}</td><td>{entity['text']}</td><td>{source}</td><td>{confidence:.1f}%</td>\"\n",
        "        entity_table += \"</tr>\"\n",
        "\n",
        "    entity_table += \"</table>\"\n",
        "\n",
        "    # Crea tabella separata per le entità aggiunte da LLM\n",
        "    llm_contribution_table = \"\"\n",
        "    if use_llm and added_entities:\n",
        "        llm_contribution_table = \"<h3>Entità Aggiunte da LLM/Pattern Matching</h3>\"\n",
        "        llm_contribution_table += \"<table style='width:100%; border-collapse: collapse;'>\"\n",
        "        llm_contribution_table += \"<tr style='background-color: #f2f2f2;'><th>Tipo</th><th>Testo</th><th>Fonte</th><th>Motivo</th></tr>\"\n",
        "\n",
        "        # Ordina le entità aggiunte per tipo e poi per testo\n",
        "        sorted_added = sorted(added_entities, key=lambda x: (x[\"entity_type\"], x[\"text\"]))\n",
        "\n",
        "        for entity in sorted_added:\n",
        "            entity_type = entity[\"entity_type\"]\n",
        "            source = entity.get(\"source\", \"unknown\")\n",
        "            reason = entity.get(\"reason\", \"\")\n",
        "\n",
        "            # Colore di sfondo in base alla fonte\n",
        "            bg_color = \"#ffffff\"\n",
        "            if source == \"llm\":\n",
        "                bg_color = \"#f6ffed\"  # Verde chiaro\n",
        "            elif source == \"regex\":\n",
        "                bg_color = \"#fff7e6\"  # Giallo chiaro\n",
        "\n",
        "            llm_contribution_table += f\"<tr style='background-color: {bg_color};'>\"\n",
        "            llm_contribution_table += f\"<td><b>{entity_type}</b></td><td>{entity['text']}</td><td>{source}</td><td>{reason}</td>\"\n",
        "            llm_contribution_table += \"</tr>\"\n",
        "\n",
        "        llm_contribution_table += \"</table>\"\n",
        "    elif use_llm:\n",
        "        llm_contribution_table = \"<p><i>Il modello LLM non ha identificato entità aggiuntive.</i></p>\"\n",
        "\n",
        "    # Statistiche di performance\n",
        "    stats = {\n",
        "        \"totalEntities\": len(entities),\n",
        "        \"originalEntities\": len(entities) - len(added_entities),\n",
        "        \"addedEntities\": len(added_entities),\n",
        "        \"entityTypes\": [{\"type\": t, \"count\": c, \"name\": entity_names.get(t, t)} for t, c in type_count.items()],\n",
        "        \"sourceCounts\": source_count,\n",
        "        \"averageConfidence\": avg_confidence,\n",
        "        \"processingTimes\": {\n",
        "            \"stanford\": f\"{stanford_time:.2f}s\",\n",
        "            \"llm\": f\"{llm_time:.2f}s\",\n",
        "            \"total\": f\"{stanford_time + llm_time:.2f}s\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Combina le visualizzazioni\n",
        "    combined_html = highlighted + \"<br><br><h3>Dettaglio Entità Rilevate</h3>\" + entity_table + \"<br><br>\" + llm_contribution_table\n",
        "\n",
        "    # Restituisci i risultati\n",
        "    return combined_html, anonymized, json.dumps(stats, indent=2)\n",
        "\n",
        "# Esempi di testo\n",
        "default_general_text = \"Ciao, mi chiamo Mario Rossi e la mia email è mario.rossi@example.com. Puoi contattarmi al numero +39 123 456 7890.\"\n",
        "\n",
        "default_medical_text = \"Il paziente Mario Bianchi, nato il 15/04/1978, di 47 anni, è stato ricoverato presso l'Ospedale San Raffaele il 23/03/2025 con numero di cartella MED-12345678. Il Dr. Carlo Verdi ha diagnosticato un'ipertensione di grado moderato. Contattare il paziente al numero 333-1234567 o all'email mario.bianchi@email.it per il follow-up.\"\n",
        "\n",
        "default_financial_text = \"Gentile Cliente Antonio Neri (CF: NRENNTN80A01H501Z), la informiamo che l'accredito di €1.250,00 è stato eseguito sul suo conto IBAN: IT60X0542811101000000123456. Per ulteriori informazioni contatti il numero verde 800-123456 o passi presso la nostra filiale in Via Roma 123, Milano.\"\n",
        "\n",
        "# Inizializza i modelli\n",
        "stanford_model = StanfordDeidentifier()\n",
        "mistral_pipe = initialize_mistral()\n",
        "anonymizer = AnonymizerEngine()\n",
        "\n",
        "# Interfaccia Gradio\n",
        "demo = gr.Interface(\n",
        "    fn=process_text,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Testo da analizzare\", value=default_financial_text, lines=6),\n",
        "        gr.Radio([\"replace\", \"redact\", \"pseudonymize\"], label=\"Tipo di anonimizzazione\", value=\"replace\"),\n",
        "        gr.Slider(minimum=0.1, maximum=1.0, value=0.5, step=0.05, label=\"Soglia di confidenza\"),\n",
        "        gr.Checkbox(label=\"Utilizza LLM per migliorare il rilevamento\", value=True, info=\"Usa Mistral o altro LLM disponibile per trovare entità aggiuntive\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.HTML(label=\"Entità rilevate con dettagli\"),\n",
        "        gr.Textbox(label=\"Testo anonimizzato\", lines=6),\n",
        "        gr.JSON(label=\"Statistiche\")\n",
        "    ],\n",
        "    title=\"Sistema Ibrido di De-identificazione (Stanford + LLM)\",\n",
        "    description=\"\"\"Analizza e anonimizza testi utilizzando un approccio ibrido: prima con Stanford Deidentifier, poi raffinando i risultati con un LLM.\n",
        "    <br><b>Nota sui colori:</b> <span style=\"background-color:#e6f7ff; padding:2px\">Azzurro</span> = Stanford, <span style=\"background-color:#f6ffed; padding:2px\">Verde</span> = LLM, <span style=\"background-color:#fff7e6; padding:2px\">Giallo</span> = Pattern matching\"\"\",\n",
        "    examples=[\n",
        "        [default_general_text, \"replace\", 0.5, True],\n",
        "        [default_medical_text, \"pseudonymize\", 0.4, True],\n",
        "        [default_financial_text, \"redact\", 0.5, True],\n",
        "        [\"Mario Bianchi, residente in Via Garibaldi 35, 20121 Milano, ha acquistato una nuova Fiat Panda con targa AB123CD.\", \"pseudonymize\", 0.5, True]\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Avvia l'interfaccia\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True, share=True)"
      ]
    }
  ]
}